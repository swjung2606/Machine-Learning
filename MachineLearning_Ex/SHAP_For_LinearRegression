# -*- coding: utf-8 -*-
"""
Created on Mon Jan 16 09:40:49 2023

@author: sunjung
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
import shap


dict_data = {'x' : [1,1,1,2,2,2,3,3,3] , 'y' : [1,2,3,1,2,3,1,2,3] , 'z' : [1.59,0.47, -0.53, 3.07, 2.1, 0.87, 4.39, 3.51, 2.57]}
df = pd.DataFrame(dict_data)

# XX : features , yy : dependent variable (Target)
XX = df[['x','y']].values
yy = df['z'].values

# a + bx + cy model
poly1 = PolynomialFeatures(degree=1)
X_poly1 = pd.DataFrame(poly1.fit_transform(XX))

# fitting
model = LinearRegression()
model.fit(X_poly1 , yy)


### SHAP computation & Visualization
shap.initjs()
explainer = shap.LinearExplainer(model, X_poly1)
shap_values = explainer.shap_values(X_poly1)

# Summary Plot
fig = plt.figure()
shap.summary_plot(shap_values, X_poly1, show = False)
plt.title("Hi")
plt.show()

fig = plt.figure()
shap.summary_plot(shap_values, X_poly1, plot_type="bar", show = False)
plt.title("Hihihi")
plt.show()

# Get Feature Importance value
vals= np.abs(shap_values).mean(0)
feature_importance = pd.DataFrame(list(zip(X_poly1.columns,vals)),columns=['col_name','feature_importance_vals'])
feature_importance.sort_values(by=['feature_importance_vals'],ascending=False,inplace=True)
